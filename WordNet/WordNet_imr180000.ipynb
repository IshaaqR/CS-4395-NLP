{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ab6f8a4",
   "metadata": {},
   "source": [
    "# <center>Portfolio Assignment: WordNet</center>\n",
    "\n",
    "## 1\n",
    "WordNet is a hierarchal organization of different parts of speech. The purpose of WordNet is to reflect how people organize words in their own minds in the form of a database. Common relationships between words in WordNet include synonymy, hyperonymy, etc)\n",
    "\n",
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dadaac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('trunk.n.01'),\n",
       " Synset('trunk.n.02'),\n",
       " Synset('torso.n.01'),\n",
       " Synset('luggage_compartment.n.01'),\n",
       " Synset('proboscis.n.02')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "wn.synsets('trunk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f23e234",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c6424a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definitions: compartment in an automobile that carries luggage or shopping or tools \n",
      "\n",
      "Usage Examples: ['he put his golf bag in the trunk'] \n",
      "\n",
      "Lemmas: [Lemma('luggage_compartment.n.01.luggage_compartment'), Lemma('luggage_compartment.n.01.automobile_trunk'), Lemma('luggage_compartment.n.01.trunk')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Definitions:', wn.synset('luggage_compartment.n.01').definition(),'\\n')\n",
    "print('Usage Examples:', wn.synset('luggage_compartment.n.01').examples(),'\\n')\n",
    "print('Lemmas:', wn.synset('luggage_compartment.n.01').lemmas(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e288d235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('compartment.n.02'),\n",
       " Synset('room.n.01'),\n",
       " Synset('area.n.05'),\n",
       " Synset('structure.n.01'),\n",
       " Synset('artifact.n.01'),\n",
       " Synset('whole.n.02'),\n",
       " Synset('object.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('entity.n.01')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunk = wn.synset('luggage_compartment.n.01')\n",
    "hyper = lambda s: s.hypernyms()\n",
    "list(trunk.closure(hyper))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cae889",
   "metadata": {},
   "source": [
    "For nouns, the WordNet hierarchy begins with entity, goes down to the type of entity, then continues to go down until it reaches the actual noun itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1bd8d9",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84123a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypernyms: [Synset('compartment.n.02')]\n",
      "Hyponyms: [Synset('boot.n.02')]\n",
      "Meronyms: []\n",
      "Holonyms: [Synset('car.n.01')]\n",
      "Antonyms: [[], [], []]\n"
     ]
    }
   ],
   "source": [
    "print('Hypernyms:', wn.synset('luggage_compartment.n.01').hypernyms())\n",
    "print('Hyponyms:', wn.synset('luggage_compartment.n.01').hyponyms())\n",
    "print('Meronyms:', wn.synset('luggage_compartment.n.01').part_meronyms())\n",
    "print('Holonyms:', wn.synset('luggage_compartment.n.01').part_holonyms())\n",
    "print('Antonyms:', [l.antonyms() for l in wn.synset('luggage_compartment.n.01').lemmas()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207cedc7",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e91a4d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('lie.n.01'),\n",
       " Synset('lie.n.02'),\n",
       " Synset('lie.n.03'),\n",
       " Synset('lie.v.01'),\n",
       " Synset('lie.v.02'),\n",
       " Synset('dwell.v.02'),\n",
       " Synset('lie.v.04'),\n",
       " Synset('lie.v.05'),\n",
       " Synset('lie.v.06'),\n",
       " Synset('lie_down.v.01')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('lie')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3a027b",
   "metadata": {},
   "source": [
    "## 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8c39a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definitions: tell an untruth; pretend with intent to deceive \n",
      "\n",
      "Usage Examples: [\"Don't lie to your parents\", 'She lied when she told me she was only 29'] \n",
      "\n",
      "Lemmas: [Lemma('lie.v.05.lie')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Definitions:', wn.synset('lie.v.05').definition(),'\\n')\n",
    "print('Usage Examples:', wn.synset('lie.v.05').examples(),'\\n')\n",
    "print('Lemmas:', wn.synset('lie.v.05').lemmas(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fe99a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('misinform.v.01'),\n",
       " Synset('inform.v.01'),\n",
       " Synset('communicate.v.02'),\n",
       " Synset('interact.v.01'),\n",
       " Synset('act.v.01')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunk = wn.synset('lie.v.05')\n",
    "hyper = lambda s: s.hypernyms()\n",
    "list(trunk.closure(hyper))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd63869d",
   "metadata": {},
   "source": [
    "Unlike nouns, there is no top-level synset for verbs. Instead, the hierarchy continues up only until the high possible level that it can."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a2cc7e",
   "metadata": {},
   "source": [
    "## 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f3e5aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lie'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.morphy('lie', wn.VERB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce9df0d",
   "metadata": {},
   "source": [
    "## 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f26e70be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wu-Palmer Similarity: 0.8125\n",
      "Lesk: Synset('panther.n.02')\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "\n",
    "cougar = wn.synset('cougar.n.01')\n",
    "panther = wn.synset('panther.n.02')\n",
    "\n",
    "print('Wu-Palmer Similarity:', wn.wup_similarity(cougar, panther))\n",
    "\n",
    "sent = ['Cougars are smaller than lions, but are still dangerous']\n",
    "print('Lesk:', lesk(sent, 'panther'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f048744",
   "metadata": {},
   "source": [
    "The Wu-Palmer Similarity metric and the Lesk algorithm are both effective at capturing the similarity of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76614699",
   "metadata": {},
   "source": [
    "## 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c0e4bc",
   "metadata": {},
   "source": [
    "SentiWord is a resource built for sentiment analysis. The way it works is that it assigns positive, negative, and objective scores to synsets from WordNet that can be used to to understand someone's opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a3a93b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\iront\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hate.n.01: PosScore=0.125 NegScore=0.375>\n",
      "Positive Score: 0.125\n",
      "Negative Score: 0.375\n",
      "Objective Score: 0.5\n",
      "\n",
      "\n",
      "<hate.v.01: PosScore=0.0 NegScore=0.75>\n",
      "Positive Score: 0.0\n",
      "Negative Score: 0.75\n",
      "Objective Score: 0.25\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('sentiwordnet')\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "hate_list = swn.senti_synsets('hate')\n",
    "for hate in hate_list:\n",
    "    print(hate)\n",
    "    print('Positive Score:', hate.pos_score())\n",
    "    print('Negative Score:', hate.neg_score())\n",
    "    print('Objective Score:', hate.obj_score())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6785c991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<iodine.n.01: PosScore=0.0 NegScore=0.0>\n",
      "Positive Score: 0.0\n",
      "Negative Score: 0.0\n",
      "Objective Score: 1.0\n",
      "\n",
      "\n",
      "<hate.n.01: PosScore=0.125 NegScore=0.375>\n",
      "Positive Score: 0.125\n",
      "Negative Score: 0.375\n",
      "Objective Score: 0.5\n",
      "\n",
      "\n",
      "<indiana.n.01: PosScore=0.0 NegScore=0.0>\n",
      "Positive Score: 0.0\n",
      "Negative Score: 0.0\n",
      "Objective Score: 1.0\n",
      "\n",
      "\n",
      "<jones.n.01: PosScore=0.0 NegScore=0.0>\n",
      "Positive Score: 0.0\n",
      "Negative Score: 0.0\n",
      "Objective Score: 1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent = 'I hate Indiana Jones'\n",
    "tokens = sent.split()\n",
    "for token in tokens:\n",
    "    syn_list = list(swn.senti_synsets(token))\n",
    "    if syn_list:\n",
    "        print(syn_list[0])\n",
    "        print('Positive Score:', syn_list[0].pos_score())\n",
    "        print('Negative Score:', syn_list[0].neg_score())\n",
    "        print('Objective Score:', syn_list[0].obj_score())\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b591c956",
   "metadata": {},
   "source": [
    "WordNet assigns scores only to synsets that are commonly used in opinionated ways; otherwise, default scores are used. One application for these scores is summing up all the scores in a sentence in order to understand the overall polarity of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea7b70b",
   "metadata": {},
   "source": [
    "## 10\n",
    "\n",
    "A collocation is a set of words that are commonly found together, despite sometimes having very different meanings. For example \"right\" and \"now\" mean very different things, but \"right now\" is commonly said."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c179a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n",
      "United States; fellow citizens; years ago; four years; Federal\n",
      "Government; General Government; American people; Vice President; God\n",
      "bless; Chief Justice; one another; fellow Americans; Old World;\n",
      "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
      "tribes; public debt; foreign nations\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *\n",
    "\n",
    "text4.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "145afa91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(United States) =  0.015860349127182045\n",
      "p(United) =  0.0170573566084788\n",
      "p(States) =  0.03301745635910224\n",
      "pmi =  4.815657649820885\n"
     ]
    }
   ],
   "source": [
    "text = ' '.join(text4.tokens)\n",
    "\n",
    "import math\n",
    "vocab = len(set(text4))\n",
    "hg = text.count('United States')/vocab\n",
    "print(\"p(United States) = \",hg )\n",
    "h = text.count('United')/vocab\n",
    "print(\"p(United) = \", h)\n",
    "g = text.count('States')/vocab\n",
    "print('p(States) = ', g)\n",
    "pmi = math.log2(hg / (h * g))\n",
    "print('pmi = ', pmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12238670",
   "metadata": {},
   "source": [
    "While simple, the formula for point-wise mutual information is very effective and can tell us which words are likely to be collocations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
